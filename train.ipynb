{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, Literal\n",
    "from ip_adapter.ip_adapter import Resampler\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import json\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, StableDiffusionXLControlNetInpaintPipeline\n",
    "from transformers import AutoTokenizer, PretrainedConfig,CLIPImageProcessor, CLIPVisionModelWithProjection,CLIPTextModelWithProjection, CLIPTextModel, CLIPTokenizer\n",
    "import cv2\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from numpy.linalg import lstsq\n",
    "import yaml\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "os.chdir('/home/bala/Desktop/sri_krishna/IDM-VTON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map={\n",
    "    \"background\": 0,\n",
    "    \"hat\": 1,\n",
    "    \"hair\": 2,\n",
    "    \"sunglasses\": 3,\n",
    "    \"upper_clothes\": 4,\n",
    "    \"skirt\": 5,\n",
    "    \"pants\": 6,\n",
    "    \"dress\": 7,\n",
    "    \"belt\": 8,\n",
    "    \"left_shoe\": 9,\n",
    "    \"right_shoe\": 10,\n",
    "    \"head\": 11,\n",
    "    \"left_leg\": 12,\n",
    "    \"right_leg\": 13,\n",
    "    \"left_arm\": 14,\n",
    "    \"right_arm\": 15,\n",
    "    \"bag\": 16,\n",
    "    \"scarf\": 17,\n",
    "}\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "#     parser.add_argument(\"--pretrained_model_name_or_path\",type=str,default= \"yisol/IDM-VTON\",required=False,)\n",
    "#     parser.add_argument(\"--width\",type=int,default=768,)\n",
    "#     parser.add_argument(\"--height\",type=int,default=1024,)\n",
    "#     parser.add_argument(\"--num_inference_steps\",type=int,default=30,)\n",
    "#     parser.add_argument(\"--output_dir\",type=str,default=\"result\",)\n",
    "#     parser.add_argument(\"--category\",type=str,default=\"upper_body\",choices=[\"upper_body\", \"lower_body\", \"dresses\"])\n",
    "#     parser.add_argument(\"--unpaired\",action=\"store_true\",)\n",
    "#     parser.add_argument(\"--data_dir\",type=str,default=\"archive\")\n",
    "#     parser.add_argument(\"--seed\", type=int, default=42,)\n",
    "#     parser.add_argument(\"--train_batch_size\", type=int, default=8,)\n",
    "#     parser.add_argument(\"--train_epochs\", type=int, default=10,)\n",
    "#     parser.add_argument(\"--test_batch_size\", type=int, default=2,)\n",
    "#     parser.add_argument(\"--guidance_scale\",type=float,default=2.0,)\n",
    "#     parser.add_argument(\"--mixed_precision\",type=str,default=None,choices=[\"no\", \"fp16\", \"bf16\"],)\n",
    "#     parser.add_argument(\"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "\n",
    "#     return args\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "with open('viton_train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "args = config\n",
    "if config[\"seed\"] is not None:\n",
    "        set_seed(args[\"seed\"])\n",
    "\n",
    "def pil_to_tensor(images):\n",
    "    images = np.array(images).astype(np.float32) / 255.0\n",
    "    images = torch.from_numpy(images.transpose(2, 0, 1))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitonHDDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataroot_path: str,\n",
    "        transformations ,\n",
    "        phase: Literal[\"train\", \"test\"],\n",
    "        order: Literal[\"paired\", \"unpaired\"] = \"paired\",\n",
    "        size: Tuple[int, int] = (512, 384),\n",
    "        \n",
    "    ):\n",
    "        super(VitonHDDataset, self).__init__()\n",
    "        self.dataroot = dataroot_path\n",
    "        self.phase = phase\n",
    "        self.height = size[0]\n",
    "        self.width = size[1]\n",
    "        self.size = size\n",
    "        self.transform = transformations\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "        self.annotation_pair = {}\n",
    "        try : \n",
    "            with open(\n",
    "                os.path.join(dataroot_path, phase, \"vitonhd_\" + phase + \"_tagged.json\"), \"r\"\n",
    "            ) as file1:\n",
    "                data1 = json.load(file1)\n",
    "\n",
    "            annotation_list = [\n",
    "                \"sleeveLength\",\n",
    "                \"neckLine\",\n",
    "                \"item\",\n",
    "            ]\n",
    "\n",
    "            \n",
    "            for k, v in data1.items():\n",
    "                for elem in v:\n",
    "                    annotation_str = \"\"\n",
    "                    for template in annotation_list:\n",
    "                        for tag in elem[\"tag_info\"]:\n",
    "                            if (\n",
    "                                tag[\"tag_name\"] == template\n",
    "                                and tag[\"tag_category\"] is not None\n",
    "                            ):\n",
    "                                annotation_str += tag[\"tag_category\"]\n",
    "                                annotation_str += \" \"\n",
    "                    self.annotation_pair[elem[\"file_name\"]] = annotation_str\n",
    "        except:\n",
    "            print(f\"No annotation file found for {self.phase} phase in {self.dataroot}\")                                  \n",
    "                                  \n",
    "        \n",
    "\n",
    "        self.order = order\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "        im_names = []\n",
    "        c_names = []\n",
    "        dataroot_names = []\n",
    "\n",
    "\n",
    "        if phase == \"train\":\n",
    "            filename = os.path.join(dataroot_path, f\"{phase}_pairs.txt\")\n",
    "        else:\n",
    "            filename = os.path.join(dataroot_path, f\"{phase}_pairs.txt\")\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                if phase == \"train\":\n",
    "                    im_name, _ = line.strip().split()\n",
    "                    c_name = im_name\n",
    "                else:\n",
    "                    if order == \"paired\":\n",
    "                        im_name, _ = line.strip().split()\n",
    "                        c_name = im_name\n",
    "                    else:\n",
    "                        im_name, c_name = line.strip().split()\n",
    "\n",
    "                im_names.append(im_name)\n",
    "                c_names.append(c_name)\n",
    "                dataroot_names.append(dataroot_path)\n",
    "\n",
    "        self.im_names = im_names\n",
    "        self.c_names = c_names\n",
    "        self.dataroot_names = dataroot_names\n",
    "        self.clip_processor = CLIPImageProcessor()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        c_name = self.c_names[index]\n",
    "        im_name = self.im_names[index]\n",
    "        if c_name in self.annotation_pair:\n",
    "            cloth_annotation = self.annotation_pair[c_name]\n",
    "        else:\n",
    "            cloth_annotation = \"shirts\"\n",
    "        cloth = Image.open(os.path.join(self.dataroot, self.phase, \"cloth\", c_name))\n",
    "\n",
    "        im_pil_big = Image.open(\n",
    "            os.path.join(self.dataroot, self.phase, \"image\", im_name)\n",
    "        ).resize((self.width,self.height))\n",
    "        image = self.transform(im_pil_big)\n",
    "\n",
    "        mask = Image.open(os.path.join(self.dataroot, self.phase, \"agnostic-mask\", im_name)).resize((self.width,self.height))\n",
    "        mask = self.toTensor(mask)\n",
    "        mask = mask[:1]\n",
    "        mask = 1-mask\n",
    "        im_mask = image * mask\n",
    " \n",
    "        pose_img = Image.open(\n",
    "            os.path.join(self.dataroot, self.phase, \"image-densepose\", im_name)\n",
    "        )\n",
    "        pose_img = self.transform(pose_img)  # [-1,1]\n",
    " \n",
    "        result = {}\n",
    "        result[\"c_name\"] = c_name\n",
    "        result[\"im_name\"] = im_name\n",
    "        result[\"image\"] = image\n",
    "        result[\"cloth_pure\"] = self.transform(cloth).half()\n",
    "\n",
    "        result[\"cloth\"] = self.clip_processor(images=cloth, return_tensors=\"pt\").pixel_values.half()\n",
    "\n",
    "        result[\"inpaint_mask\"] =1-mask\n",
    "        result[\"im_mask\"] = im_mask\n",
    "        result[\"caption_cloth\"] = \"a photo of \" + cloth_annotation\n",
    "        result[\"caption\"] = \"model is wearing a \" + cloth_annotation\n",
    "        result[\"pose_img\"] = pose_img\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        # model images + cloth image\n",
    "        return len(self.im_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Train Images to 4000 to train faster\n",
    "\n",
    "- In below code I tried to reduce img size to be able to afford higher batch_size....\n",
    "- But the model only accepts img of size (768X1024)\n",
    "\n",
    "- But I reduced the training set from ~11k to 4k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define the transformations\n",
    "\n",
    "def reduce_pixels(img_pth):\n",
    "    transformations = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((400, 280)),\n",
    "            # transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    image = Image.open(img_pth).convert('RGB')  # Convert to grayscale\n",
    "\n",
    "    # Apply the transformations\n",
    "    transformed_image = transformations(image)\n",
    "    transformed_image = transforms.ToPILImage()(transformed_image)\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "if not os.path.exists('archive/train/image'):\n",
    "    os.makedirs('archive/train/image')\n",
    "    os.makedirs('archive/train/cloths')\n",
    "    \n",
    "    for f_name in os.listdir('archive/train/image_original'):\n",
    "        img = reduce_pixels(os.path.join('archive/train/image_original', f_name))\n",
    "        img.save(f'archive/train/image/{f_name}')\n",
    "        \n",
    "    for f_name in os.listdir('archive/train/cloth_original'):\n",
    "        img = reduce_pixels(os.path.join('archive/train/cloth_original', f_name))\n",
    "        img.save(f'archive/train/cloths/{f_name}')\n",
    "\n",
    "    if not os.path.exists('archive/train_pairs.txt'):\n",
    "        NUM = 4000\n",
    "\n",
    "        with open('archive/train_pairs_original.txt', 'r') as f:\n",
    "            train_pairs = f.readlines()\n",
    "            \n",
    "        with open('archive/train_pairs.txt', 'w') as f:\n",
    "            #select 4000 random pairs\n",
    "            indices = np.random.choice(len(train_pairs), NUM, replace=False)\n",
    "            for i in indices:\n",
    "                f.write(train_pairs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No annotation file found for train phase in archive\n"
     ]
    }
   ],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")   \n",
    "\n",
    "\n",
    "train_dataset = VitonHDDataset(\n",
    "        dataroot_path=args[\"data_dir\"],\n",
    "        transformations=transformations,\n",
    "        phase=\"train\",\n",
    "        order=\"unpaired\" if args[\"unpaired\"] else \"paired\",\n",
    "        size=(args[\"height\"], args[\"width\"]),\n",
    "    )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=args[\"train_batch_size\"],\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_dataset = VitonHDDataset(\n",
    "        dataroot_path=args[\"data_dir\"],\n",
    "        transformations=transformations,\n",
    "        phase=\"test\",\n",
    "        order=\"unpaired\" if args[\"unpaired\"] else \"paired\",\n",
    "        size=(args[\"height\"], args[\"width\"]),\n",
    "    )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=args[\"test_batch_size\"],\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    # if accelerator.mixed_precision == \"fp16\":\n",
    "    #     weight_dtype = torch.float16\n",
    "    #     args.mixed_precision = accelerator.mixed_precision\n",
    "    # elif accelerator.mixed_precision == \"bf16\":\n",
    "    #     weight_dtype = torch.bfloat16\n",
    "    #     args.mixed_precision = accelerator.mixed_precision\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    \n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args[\"output_dir\"])\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args[\"mixed_precision\"],\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n",
    "    # If passed along, set the training seed now.\n",
    "    \n",
    "\n",
    "    # Handle the repository creation\n",
    "    if accelerator.is_main_process:\n",
    "        if args[\"output_dir\"] is not None:\n",
    "            os.makedirs(args[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "    weight_dtype = torch.float16\n",
    "    \n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args[\"pretrained_model_name_or_path\"], subfolder=\"scheduler\")\n",
    "    vae = AutoencoderKL.from_pretrained(                           # VAE (frozen pre-trained VAE) for latent space creation from images\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(                     # TryonNet (trainable Unet)\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(     # IP adapter (trainable)\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"image_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(        # GarmentNet (frozen pre-trained UNet encoder)\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"unet_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"text_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    text_encoder_two = CLIPTextModelWithProjection.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # tokenizers for text encoders\n",
    "    tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"tokenizer_2\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "                \n",
    "    unet.requires_grad_(True)\n",
    "    vae.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(True)\n",
    "    UNet_Encoder.requires_grad_(False)\n",
    "    text_encoder_one.requires_grad_(False)\n",
    "    text_encoder_two.requires_grad_(False)\n",
    "    UNet_Encoder.to(accelerator.device, weight_dtype)\n",
    "    # unet.eval()\n",
    "    UNet_Encoder.eval()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"accelerator\": accelerator,\n",
    "        \"noise_scheduler\": noise_scheduler,\n",
    "        \"vae\": vae,\n",
    "        \"unet\": unet,\n",
    "        \"image_encoder\": image_encoder,\n",
    "        \"UNet_Encoder\": UNet_Encoder,\n",
    "        \"text_encoder_one\": text_encoder_one,\n",
    "        \"text_encoder_two\": text_encoder_two,\n",
    "        \"tokenizer_one\": tokenizer_one,\n",
    "        \"tokenizer_two\": tokenizer_two,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    models = get_models()\n",
    "\n",
    "    if args[\"enable_xformers_memory_efficient_attention\"]:\n",
    "        if is_xformers_available():\n",
    "            import xformers\n",
    "\n",
    "            xformers_version = version.parse(xformers.__version__)\n",
    "            if xformers_version == version.parse(\"0.0.16\"):\n",
    "                logger.warn(\n",
    "                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n",
    "                )\n",
    "            models[\"unet\"].enable_xformers_memory_efficient_attention()\n",
    "        else:\n",
    "            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "    \n",
    "\n",
    "    pipe = TryonPipeline.from_pretrained(\n",
    "            args[\"pretrained_model_name_or_path\"],\n",
    "            unet=models[\"unet\"],\n",
    "            vae=models[\"vae\"],\n",
    "            feature_extractor= CLIPImageProcessor(),\n",
    "            text_encoder = models[\"text_encoder_one\"],\n",
    "            text_encoder_2 = models[\"text_encoder_two\"],\n",
    "            tokenizer = models[\"tokenizer_one\"],\n",
    "            tokenizer_2 = models[\"tokenizer_two\"],\n",
    "            scheduler = models[\"noise_scheduler\"],\n",
    "            image_encoder=models[\"image_encoder\"],\n",
    "            torch_dtype=torch.float16,\n",
    "    ).to(models[\"accelerator\"].device)\n",
    "    pipe.unet_encoder = models[\"UNet_Encoder\"]\n",
    "    \n",
    "    return  pipe, models\n",
    "\n",
    "    # pipe.enable_sequential_cpu_offload()\n",
    "    # pipe.enable_model_cpu_offload()\n",
    "    # pipe.enable_vae_slicing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions : \n",
    "\n",
    "There are various loss functions available for training diffusion models like : \n",
    "#### L1 loss\n",
    "#### LPIPS\n",
    "A lower LPIPS score indicates that the generated image is more similar \n",
    "to the real image, and thus the diffusion model is performing better.\n",
    "It uses a deep neural network (specifically, a version of the AlexNet or VGG network) to extract features \n",
    "from image patches, and computes distances in this feature space to measure image similarity. \n",
    "\n",
    "#### SSIM\n",
    "A higher SSIM score indicates that the generated image is more similar to the real image, \n",
    "and thus the diffusion model is performing better.\n",
    "\n",
    " SSIM considers changes in structural information, luminance, and contrast of the images.  \n",
    "\n",
    "##### **I chosen to move with L1 loss.... But any one can be placed in training loop in place of L1 loss**      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import lpips\n",
    "# import torch\n",
    "\n",
    "# # Initialize the LPIPS metric\n",
    "# loss_fn_vgg = lpips.LPIPS(net='vgg')\n",
    "\n",
    "# # Define your input and target tensors\n",
    "# input = torch.randn(1, 3, 64, 64)\n",
    "# target = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# # Compute the LPIPS distance\n",
    "# distance = loss_fn_vgg(input, target)\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "\n",
    "# from pytorch_msssim import ssim\n",
    "# import torch\n",
    "\n",
    "# # Define your input and target tensors\n",
    "# input = torch.randn(1, 1, 256, 256)\n",
    "# target = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "# # Compute the SSIM\n",
    "# ssim_value = ssim(input, target, data_range=1.0)\n",
    "\n",
    "# print(\"SSIM: \", ssim_value.item())\n",
    "\n",
    "#____________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Model passes and evaluation\n",
    "\n",
    "**Training Scenario**\n",
    "* Since images have high resolution, I could only keep a batch size of 2 (OutOfMemory error beyond that)\n",
    "* To increase effective batch size, I set the **accumulate_grad_batches = 50** ... Making gradient updates after 50 steps\n",
    "* As per the paper, they only trained **TryonNet** and **IPAdapter**, kept rest of the models frozen .... I followed the approach\n",
    "* I have also employed learning rate scheduler under optimizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TrainEval(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        self.config = config  \n",
    "        self.pipe, self.models = main()\n",
    "        self.log_every_n_steps = self.config['accumulate_grad_batches']\n",
    "\n",
    "    def get_output(self, batch):\n",
    "        img_emb_list = []\n",
    "        for i in range(batch['cloth'].shape[0]):\n",
    "            img_emb_list.append(batch['cloth'][i])\n",
    "        \n",
    "        prompt = batch[\"caption\"]\n",
    "\n",
    "        num_prompts = batch['cloth'].shape[0]                                        \n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_embeds = torch.cat(img_emb_list,dim=0)\n",
    "        # with torch.inference_mode():\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.pipe.encode_prompt(\n",
    "            prompt,\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=True,\n",
    "            negative_prompt=negative_prompt,\n",
    "        )\n",
    "    \n",
    "        prompt = batch[\"caption_cloth\"]\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        # with torch.inference_mode():\n",
    "        (\n",
    "            prompt_embeds_c,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = self.pipe.encode_prompt(\n",
    "            prompt,\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=False,\n",
    "            negative_prompt=negative_prompt,\n",
    "        )\n",
    "\n",
    "\n",
    "        generator = torch.Generator(self.pipe.device).manual_seed(args[\"seed\"]) if args[\"seed\"] is not None else None\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            num_inference_steps=args[\"num_inference_steps\"],\n",
    "            generator=generator,\n",
    "            strength = 1.0,\n",
    "            pose_img = batch['pose_img'],\n",
    "            text_embeds_cloth=prompt_embeds_c,\n",
    "            cloth = batch[\"cloth_pure\"].to(self.models[\"accelerator\"].device),\n",
    "            mask_image=batch['inpaint_mask'],\n",
    "            image=(batch['image']+1.0)/2.0, \n",
    "            height=args[\"height\"],\n",
    "            width=args[\"width\"],\n",
    "            guidance_scale=args[\"guidance_scale\"],\n",
    "            ip_adapter_image = image_embeds,\n",
    "        )[0]\n",
    "        \n",
    "        to_tensor = transforms.ToTensor()\n",
    "\n",
    "        # Convert list of PIL images to list of tensors\n",
    "        tensor_images = [to_tensor(image) for image in images]\n",
    "\n",
    "        # Stack the list of tensors into a single tensor\n",
    "        stacked_tensor_images = torch.stack(tensor_images)\n",
    "        # print(60)\n",
    "        # print(type(images))\n",
    "        # print(type(images[0]))\n",
    "        return stacked_tensor_images.to(self.models[\"accelerator\"].device)\n",
    "\n",
    "  \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        generated_images = self.get_output(batch)\n",
    "        generated_images.requires_grad = True\n",
    "        loss = self.criterion(generated_images, batch['image'].to(self.models[\"accelerator\"].device))\n",
    "        # if (self.global_step + 1) % self.log_every_n_steps == 0:\n",
    "        self.log(\"train_loss\", loss,on_step = True ,on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        generated_images = self.get_output(batch)\n",
    "        loss = self.criterion(generated_images, batch['image'].to(self.models[\"accelerator\"].device))\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        generated_images = self.get_output(batch)\n",
    "        loss = self.criterion(generated_images, batch['image'].to(self.models[\"accelerator\"].device))\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        \n",
    "        for i in range(len(generated_images)):\n",
    "            x_sample = pil_to_tensor(generated_images[i])\n",
    "            torchvision.utils.save_image(x_sample,os.path.join(args[\"output_dir\"],batch['im_name'][i]))\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = []\n",
    "        for param in self.models[\"unet\"].parameters():\n",
    "            parameters.append(param)\n",
    "            assert param.requires_grad, \"All parameters should require gradients.\"\n",
    "            \n",
    "        for param in self.models[\"image_encoder\"].parameters():\n",
    "            parameters.append(param)\n",
    "            assert param.requires_grad, \"All parameters should require gradients.\"\n",
    "            \n",
    "        optim =  torch.optim.Adam(params = parameters, lr = self.config['lr'], weight_decay = self.config['weight_decay'])   # https://pytorch.org/docs/stable/optim.html\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=3, factor=0.7, \n",
    "                                                                  threshold=0.005, cooldown =2,verbose=True)\n",
    "        # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optim,gamma = 0.995 ,last_epoch=-1,   verbose=True)\n",
    "\n",
    "        return [optim], [{'scheduler': lr_scheduler, 'interval': 'epoch', 'monitor': 'train_loss', 'name': 'lr_scheduler'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar, RichModelSummary\n",
    "from torchvision import transforms\n",
    "\n",
    "from  pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.00001,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "theme = RichProgressBarTheme(metrics='green', time='yellow', progress_bar_finished='#8c53e0' ,progress_bar='#c99e38')\n",
    "rich_progress_bar = RichProgressBar(theme=theme)\n",
    "\n",
    "# rich_model_summary = RichModelSummary(max_depth=5)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=3,\n",
    "    verbose=True,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "Here model and data-loaders meet to train and evaluate of dataset....\n",
    "\n",
    "* I set it for training 10 epochs... But one epoch took more than 1 day to complete.... I need to terminate it \n",
    "* Logging is done to wandb... while running below cell it would ask for wandb API key just for intial run.\n",
    "* checkpoints are also saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import yaml \n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "#___________________________________________________________________________________________________________________\n",
    "model = TrainEval(config)\n",
    "\n",
    "NAME = config['model_name']\n",
    "checkpoint_callback.dirpath = os.path.join(config['dir'], 'ckpts')\n",
    "checkpoint_callback.filename = NAME+'__' + config['ckpt_file_name']\n",
    "\n",
    "run_name = f\"lr_{config['lr']} *** bs{config['train_batch_size']} *** decay_{config['weight_decay']}\"\n",
    "wandb_logger = WandbLogger(project= NAME, name = run_name)\n",
    "\n",
    "trainer = Trainer(callbacks=[early_stop_callback, checkpoint_callback, rich_progress_bar], \n",
    "                  accelerator = 'gpu' ,max_epochs=args[\"train_epochs\"], logger=[wandb_logger], \n",
    "                  accumulate_grad_batches = config['accumulate_grad_batches'])  \n",
    " \n",
    "trainer.fit(model, train_dataloader, test_dataloader)\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "True\n",
      "0\n",
      "NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
