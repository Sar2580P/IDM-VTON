{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, Literal\n",
    "from ip_adapter.ip_adapter import Resampler\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import json\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, StableDiffusionXLControlNetInpaintPipeline\n",
    "from transformers import AutoTokenizer, PretrainedConfig,CLIPImageProcessor, CLIPVisionModelWithProjection,CLIPTextModelWithProjection, CLIPTextModel, CLIPTokenizer\n",
    "import cv2\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from numpy.linalg import lstsq\n",
    "\n",
    "from src.unet_hacked_tryon import UNet2DConditionModel\n",
    "from src.unet_hacked_garmnet import UNet2DConditionModel as UNet2DConditionModel_ref\n",
    "from src.tryon_pipeline import StableDiffusionXLInpaintPipeline as TryonPipeline\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "os.chdir('/home/bala/Desktop/sri_krishna/IDM-VTON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map={\n",
    "    \"background\": 0,\n",
    "    \"hat\": 1,\n",
    "    \"hair\": 2,\n",
    "    \"sunglasses\": 3,\n",
    "    \"upper_clothes\": 4,\n",
    "    \"skirt\": 5,\n",
    "    \"pants\": 6,\n",
    "    \"dress\": 7,\n",
    "    \"belt\": 8,\n",
    "    \"left_shoe\": 9,\n",
    "    \"right_shoe\": 10,\n",
    "    \"head\": 11,\n",
    "    \"left_leg\": 12,\n",
    "    \"right_leg\": 13,\n",
    "    \"left_arm\": 14,\n",
    "    \"right_arm\": 15,\n",
    "    \"bag\": 16,\n",
    "    \"scarf\": 17,\n",
    "}\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "#     parser.add_argument(\"--pretrained_model_name_or_path\",type=str,default= \"yisol/IDM-VTON\",required=False,)\n",
    "#     parser.add_argument(\"--width\",type=int,default=768,)\n",
    "#     parser.add_argument(\"--height\",type=int,default=1024,)\n",
    "#     parser.add_argument(\"--num_inference_steps\",type=int,default=30,)\n",
    "#     parser.add_argument(\"--output_dir\",type=str,default=\"result\",)\n",
    "#     parser.add_argument(\"--category\",type=str,default=\"upper_body\",choices=[\"upper_body\", \"lower_body\", \"dresses\"])\n",
    "#     parser.add_argument(\"--unpaired\",action=\"store_true\",)\n",
    "#     parser.add_argument(\"--data_dir\",type=str,default=\"archive\")\n",
    "#     parser.add_argument(\"--seed\", type=int, default=42,)\n",
    "#     parser.add_argument(\"--train_batch_size\", type=int, default=8,)\n",
    "#     parser.add_argument(\"--train_epochs\", type=int, default=10,)\n",
    "#     parser.add_argument(\"--test_batch_size\", type=int, default=2,)\n",
    "#     parser.add_argument(\"--guidance_scale\",type=float,default=2.0,)\n",
    "#     parser.add_argument(\"--mixed_precision\",type=str,default=None,choices=[\"no\", \"fp16\", \"bf16\"],)\n",
    "#     parser.add_argument(\"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "\n",
    "#     return args\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "args = {\n",
    "    \"pretrained_model_name_or_path\": \"yisol/IDM-VTON\",\n",
    "    \"width\": 768,\n",
    "    \"height\": 1024,\n",
    "    \"num_inference_steps\": 30,\n",
    "    \"output_dir\": \"result\",\n",
    "    \"category\": \"upper_body\",  # Change this value if needed (\"upper_body\", \"lower_body\", \"dresses\")\n",
    "    \"unpaired\": False,  # Set to True if the --unpaired flag should be used\n",
    "    \"data_dir\": \"archive\",\n",
    "    \"seed\": 42,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"train_epochs\": 10,\n",
    "    \"test_batch_size\": 2,\n",
    "    \"guidance_scale\": 2.0,\n",
    "    \"mixed_precision\": None,  # Change this value if needed (\"no\", \"fp16\", \"bf16\")\n",
    "    \"enable_xformers_memory_efficient_attention\": False  # Set to True if the --enable_xformers_memory_efficient_attention flag should be used\n",
    "}\n",
    "\n",
    "if args[\"seed\"] is not None:\n",
    "        set_seed(args[\"seed\"])\n",
    "\n",
    "def pil_to_tensor(images):\n",
    "    images = np.array(images).astype(np.float32) / 255.0\n",
    "    images = torch.from_numpy(images.transpose(2, 0, 1))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitonHDDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataroot_path: str,\n",
    "        transformations ,\n",
    "        phase: Literal[\"train\", \"test\"],\n",
    "        order: Literal[\"paired\", \"unpaired\"] = \"paired\",\n",
    "        size: Tuple[int, int] = (512, 384),\n",
    "        \n",
    "    ):\n",
    "        super(VitonHDDataset, self).__init__()\n",
    "        self.dataroot = dataroot_path\n",
    "        self.phase = phase\n",
    "        self.height = size[0]\n",
    "        self.width = size[1]\n",
    "        self.size = size\n",
    "        self.transform = transformations\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "        self.annotation_pair = {}\n",
    "        try : \n",
    "            with open(\n",
    "                os.path.join(dataroot_path, phase, \"vitonhd_\" + phase + \"_tagged.json\"), \"r\"\n",
    "            ) as file1:\n",
    "                data1 = json.load(file1)\n",
    "\n",
    "            annotation_list = [\n",
    "                \"sleeveLength\",\n",
    "                \"neckLine\",\n",
    "                \"item\",\n",
    "            ]\n",
    "\n",
    "            \n",
    "            for k, v in data1.items():\n",
    "                for elem in v:\n",
    "                    annotation_str = \"\"\n",
    "                    for template in annotation_list:\n",
    "                        for tag in elem[\"tag_info\"]:\n",
    "                            if (\n",
    "                                tag[\"tag_name\"] == template\n",
    "                                and tag[\"tag_category\"] is not None\n",
    "                            ):\n",
    "                                annotation_str += tag[\"tag_category\"]\n",
    "                                annotation_str += \" \"\n",
    "                    self.annotation_pair[elem[\"file_name\"]] = annotation_str\n",
    "        except:\n",
    "            print(f\"No annotation file found for {self.phase} phase in {self.dataroot}\")                                  \n",
    "                                  \n",
    "        \n",
    "\n",
    "        self.order = order\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "        im_names = []\n",
    "        c_names = []\n",
    "        dataroot_names = []\n",
    "\n",
    "\n",
    "        if phase == \"train\":\n",
    "            filename = os.path.join(dataroot_path, f\"{phase}_pairs.txt\")\n",
    "        else:\n",
    "            filename = os.path.join(dataroot_path, f\"{phase}_pairs.txt\")\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                if phase == \"train\":\n",
    "                    im_name, _ = line.strip().split()\n",
    "                    c_name = im_name\n",
    "                else:\n",
    "                    if order == \"paired\":\n",
    "                        im_name, _ = line.strip().split()\n",
    "                        c_name = im_name\n",
    "                    else:\n",
    "                        im_name, c_name = line.strip().split()\n",
    "\n",
    "                im_names.append(im_name)\n",
    "                c_names.append(c_name)\n",
    "                dataroot_names.append(dataroot_path)\n",
    "\n",
    "        self.im_names = im_names\n",
    "        self.c_names = c_names\n",
    "        self.dataroot_names = dataroot_names\n",
    "        self.clip_processor = CLIPImageProcessor()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        c_name = self.c_names[index]\n",
    "        im_name = self.im_names[index]\n",
    "        if c_name in self.annotation_pair:\n",
    "            cloth_annotation = self.annotation_pair[c_name]\n",
    "        else:\n",
    "            cloth_annotation = \"shirts\"\n",
    "        cloth = Image.open(os.path.join(self.dataroot, self.phase, \"cloth\", c_name))\n",
    "\n",
    "        im_pil_big = Image.open(\n",
    "            os.path.join(self.dataroot, self.phase, \"image\", im_name)\n",
    "        ).resize((self.width,self.height))\n",
    "        image = self.transform(im_pil_big)\n",
    "\n",
    "        mask = Image.open(os.path.join(self.dataroot, self.phase, \"agnostic-mask\", im_name)).resize((self.width,self.height))\n",
    "        mask = self.toTensor(mask)\n",
    "        mask = mask[:1]\n",
    "        mask = 1-mask\n",
    "        im_mask = image * mask\n",
    " \n",
    "        pose_img = Image.open(\n",
    "            os.path.join(self.dataroot, self.phase, \"image-densepose\", im_name)\n",
    "        )\n",
    "        pose_img = self.transform(pose_img)  # [-1,1]\n",
    " \n",
    "        result = {}\n",
    "        result[\"c_name\"] = c_name\n",
    "        result[\"im_name\"] = im_name\n",
    "        result[\"image\"] = image\n",
    "        result[\"cloth_pure\"] = self.transform(cloth)\n",
    "        result[\"cloth\"] = self.clip_processor(images=cloth, return_tensors=\"pt\").pixel_values\n",
    "        result[\"inpaint_mask\"] =1-mask\n",
    "        result[\"im_mask\"] = im_mask\n",
    "        result[\"caption_cloth\"] = \"a photo of \" + cloth_annotation\n",
    "        result[\"caption\"] = \"model is wearing a \" + cloth_annotation\n",
    "        result[\"pose_img\"] = pose_img\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        # model images + cloth image\n",
    "        return len(self.im_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No annotation file found for train phase in archive\n"
     ]
    }
   ],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")   \n",
    "\n",
    "\n",
    "train_dataset = VitonHDDataset(\n",
    "        dataroot_path=args[\"data_dir\"],\n",
    "        transformations=transformations,\n",
    "        phase=\"train\",\n",
    "        order=\"unpaired\" if args[\"unpaired\"] else \"paired\",\n",
    "        size=(args[\"height\"], args[\"width\"]),\n",
    "    )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=args[\"train_batch_size\"],\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_dataset = VitonHDDataset(\n",
    "        dataroot_path=args[\"data_dir\"],\n",
    "        transformations=transformations,\n",
    "        phase=\"test\",\n",
    "        order=\"unpaired\" if args[\"unpaired\"] else \"paired\",\n",
    "        size=(args[\"height\"], args[\"width\"]),\n",
    "    )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=args[\"test_batch_size\"],\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    # if accelerator.mixed_precision == \"fp16\":\n",
    "    #     weight_dtype = torch.float16\n",
    "    #     args.mixed_precision = accelerator.mixed_precision\n",
    "    # elif accelerator.mixed_precision == \"bf16\":\n",
    "    #     weight_dtype = torch.bfloat16\n",
    "    #     args.mixed_precision = accelerator.mixed_precision\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    \n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args[\"output_dir\"])\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args[\"mixed_precision\"],\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n",
    "    # If passed along, set the training seed now.\n",
    "    \n",
    "\n",
    "    # Handle the repository creation\n",
    "    if accelerator.is_main_process:\n",
    "        if args[\"output_dir\"] is not None:\n",
    "            os.makedirs(args[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "    weight_dtype = torch.float16\n",
    "    \n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args[\"pretrained_model_name_or_path\"], subfolder=\"scheduler\")\n",
    "    vae = AutoencoderKL.from_pretrained(                           # VAE (frozen pre-trained VAE) for latent space creation from images\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(                     # TryonNet (trainable Unet)\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"unet\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(     # IP adapter (trainable)\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"image_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    UNet_Encoder = UNet2DConditionModel_ref.from_pretrained(        # GarmentNet (frozen pre-trained UNet encoder)\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"unet_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"text_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    text_encoder_two = CLIPTextModelWithProjection.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # tokenizers for text encoders\n",
    "    tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "        args[\"pretrained_model_name_or_path\"],\n",
    "        subfolder=\"tokenizer_2\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "                \n",
    "    unet.requires_grad_(True)\n",
    "    vae.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(True)\n",
    "    UNet_Encoder.requires_grad_(False)\n",
    "    text_encoder_one.requires_grad_(False)\n",
    "    text_encoder_two.requires_grad_(False)\n",
    "    UNet_Encoder.to(accelerator.device, weight_dtype)\n",
    "    # unet.eval()\n",
    "    UNet_Encoder.eval()\n",
    "    \n",
    "    return {\n",
    "        \"accelerator\": accelerator,\n",
    "        \"noise_scheduler\": noise_scheduler,\n",
    "        \"vae\": vae,\n",
    "        \"unet\": unet,\n",
    "        \"image_encoder\": image_encoder,\n",
    "        \"UNet_Encoder\": UNet_Encoder,\n",
    "        \"text_encoder_one\": text_encoder_one,\n",
    "        \"text_encoder_two\": text_encoder_two,\n",
    "        \"tokenizer_one\": tokenizer_one,\n",
    "        \"tokenizer_two\": tokenizer_two,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    models = get_models()\n",
    "\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        if is_xformers_available():\n",
    "            import xformers\n",
    "\n",
    "            xformers_version = version.parse(xformers.__version__)\n",
    "            if xformers_version == version.parse(\"0.0.16\"):\n",
    "                logger.warn(\n",
    "                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n",
    "                )\n",
    "            models[\"unet\"].enable_xformers_memory_efficient_attention()\n",
    "        else:\n",
    "            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "    \n",
    "\n",
    "    pipe = TryonPipeline.from_pretrained(\n",
    "            args[\"pretrained_model_name_or_path\"],\n",
    "            unet=models[\"unet\"],\n",
    "            vae=models[\"vae\"],\n",
    "            feature_extractor= CLIPImageProcessor(),\n",
    "            text_encoder = models[\"text_encoder_one\"],\n",
    "            text_encoder_2 = models[\"text_encoder_two\"],\n",
    "            tokenizer = models[\"tokenizer_one\"],\n",
    "            tokenizer_2 = models[\"tokenizer_two\"],\n",
    "            scheduler = models[\"noise_scheduler\"],\n",
    "            image_encoder=models[\"image_encoder\"],\n",
    "            torch_dtype=torch.float16,\n",
    "    ).to(models[\"accelerator\"].device)\n",
    "    pipe.unet_encoder = models[\"UNet_Encoder\"]\n",
    "    \n",
    "    return  pipe, models\n",
    "\n",
    "    # pipe.enable_sequential_cpu_offload()\n",
    "    # pipe.enable_model_cpu_offload()\n",
    "    # pipe.enable_vae_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--->  A higher SSIM score indicates that the generated image is more similar to the real image, \\nand thus the diffusion model is performing better.\\n\\n SSIM considers changes in structural information, luminance, and contrast of the images.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A lower LPIPS score indicates that the generated image is more similar \n",
    "to the real image, and thus the diffusion model is performing better.\n",
    "\n",
    "It uses a deep neural network (specifically, a version of the AlexNet or VGG network) to extract features \n",
    "from image patches, and computes distances in this feature space to measure image similarity.\n",
    "'''\n",
    "# import lpips\n",
    "# import torch\n",
    "\n",
    "# # Initialize the LPIPS metric\n",
    "# loss_fn_vgg = lpips.LPIPS(net='vgg')\n",
    "\n",
    "# # Define your input and target tensors\n",
    "# input = torch.randn(1, 3, 64, 64)\n",
    "# target = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# # Compute the LPIPS distance\n",
    "# distance = loss_fn_vgg(input, target)\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "'''--->  A higher SSIM score indicates that the generated image is more similar to the real image, \n",
    "and thus the diffusion model is performing better.\n",
    "\n",
    " SSIM considers changes in structural information, luminance, and contrast of the images.\n",
    "'''\n",
    "# from pytorch_msssim import ssim\n",
    "# import torch\n",
    "\n",
    "# # Define your input and target tensors\n",
    "# input = torch.randn(1, 1, 256, 256)\n",
    "# target = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "# # Compute the SSIM\n",
    "# ssim_value = ssim(input, target, data_range=1.0)\n",
    "\n",
    "# print(\"SSIM: \", ssim_value.item())\n",
    "\n",
    "#____________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Model passes and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainEval\u001b[39;00m(pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_finder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSizeFinder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/batch_size_finder.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_scaling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _scale_batch_size\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException, _TunerExitException\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/callback.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEP_OUTPUT\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCallback\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/types.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotRequired, Required\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TORCH_LRSCHEDULER, LRScheduler, ProcessGroup, ReduceLROnPlateau\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/__init__.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m _PACKAGE_ROOT \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m     12\u001b[0m _PROJECT_ROOT \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(_PACKAGE_ROOT)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maggregation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     CatMetric,\n\u001b[1;32m     17\u001b[0m     MaxMetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     SumMetric,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _PermutationInvariantTraining \u001b[38;5;28;01mas\u001b[39;00m PermutationInvariantTraining  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/functional/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _permutation_invariant_training \u001b[38;5;28;01mas\u001b[39;00m permutation_invariant_training\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pit_permutate \u001b[38;5;28;01mas\u001b[39;00m pit_permutate\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _scale_invariant_signal_distortion_ratio \u001b[38;5;28;01mas\u001b[39;00m scale_invariant_signal_distortion_ratio,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/functional/audio/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_invariant_training, pit_permutate\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     scale_invariant_signal_distortion_ratio,\n\u001b[1;32m     17\u001b[0m     signal_distortion_ratio,\n\u001b[1;32m     18\u001b[0m     source_aggregated_signal_distortion_ratio,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     complex_scale_invariant_signal_noise_ratio,\n\u001b[1;32m     22\u001b[0m     scale_invariant_signal_noise_ratio,\n\u001b[1;32m     23\u001b[0m     signal_noise_ratio,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/functional/audio/pit.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SCIPY_AVAILABLE\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# _ps_dict: cache of permutations\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# it's necessary to cache it, otherwise it will consume a large amount of time\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/utilities/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_forward_full_state_property\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_reduce, reduce\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_debug, rank_zero_info, rank_zero_warn\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/utilities/checks.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_topk, to_onehot\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataType\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/metric.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     _flatten,\n\u001b[1;32m     32\u001b[0m     _squeeze_if_scalar,\n\u001b[1;32m     33\u001b[0m     dim_zero_cat,\n\u001b[1;32m     34\u001b[0m     dim_zero_max,\n\u001b[1;32m     35\u001b[0m     dim_zero_mean,\n\u001b[1;32m     36\u001b[0m     dim_zero_min,\n\u001b[1;32m     37\u001b[0m     dim_zero_sum,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gather_all_tensors\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchMetricsUserError\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/utilities/data.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchMetricsUserWarning\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     25\u001b[0m METRIC_EPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchmetrics/utilities/imports.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m _GAMMATONE_AVAILABEL: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgammatone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m _TORCHAUDIO_AVAILABEL: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m _TORCHAUDIO_GREATER_EQUAL_0_10: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_version\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.10.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m _SACREBLEU_AVAILABLE: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacrebleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m _REGEX_AVAILABLE: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_utilities/core/imports.py:77\u001b[0m, in \u001b[0;36mcompare_version\u001b[0;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compare package version with some requirements.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m>>> compare_version(\"torch\", operator.ge, \"0.1\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, pkg_resources\u001b[38;5;241m.\u001b[39mDistributionNotFound):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchaudio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_audio_backend, list_audio_backends, set_audio_backend\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchaudio/_extension/__init__.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m _IS_KALDI_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torchaudio/_extension/utils.py:61\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m torch\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    928\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/anaconda3/envs/idm/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/bala/anaconda3/envs/idm/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TrainEval(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        \n",
    "        self.pipe, self.models = main()\n",
    "\n",
    "    def get_output(self, batch):\n",
    "        img_emb_list = []\n",
    "        for i in range(batch['cloth'].shape[0]):\n",
    "            img_emb_list.append(batch['cloth'][i])\n",
    "        \n",
    "        prompt = batch[\"caption\"]\n",
    "\n",
    "        num_prompts = batch['cloth'].shape[0]                                        \n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_embeds = torch.cat(img_emb_list,dim=0)\n",
    "\n",
    "        # with torch.inference_mode():\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.pipe.encode_prompt(\n",
    "            prompt,\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=True,\n",
    "            negative_prompt=negative_prompt,\n",
    "        )\n",
    "    \n",
    "    \n",
    "        prompt = batch[\"caption_cloth\"]\n",
    "        negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "\n",
    "        # with torch.inference_mode():\n",
    "        (\n",
    "            prompt_embeds_c,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "        ) = self.pipe.encode_prompt(\n",
    "            prompt,\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=False,\n",
    "            negative_prompt=negative_prompt,\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        generator = torch.Generator(self.pipe.device).manual_seed(args[\"seed\"]) if args[\"seed\"] is not None else None\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            num_inference_steps=args[\"num_inference_steps\"],\n",
    "            generator=generator,\n",
    "            strength = 1.0,\n",
    "            pose_img = batch['pose_img'],\n",
    "            text_embeds_cloth=prompt_embeds_c,\n",
    "            cloth = batch[\"cloth_pure\"].to(self.models[\"accelerator\"].device),\n",
    "            mask_image=batch['inpaint_mask'],\n",
    "            image=(batch['image']+1.0)/2.0, \n",
    "            height=args[\"height\"],\n",
    "            width=args[\"width\"],\n",
    "            guidance_scale=args[\"guidance_scale\"],\n",
    "            ip_adapter_image = image_embeds,\n",
    "        )[0]\n",
    "        \n",
    "        return images\n",
    "\n",
    "  \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        generated_images = self.get_output(batch)\n",
    "        \n",
    "        loss = self.criterion(generated_images, batch['image'].to(self.models[\"accelerator\"].device))\n",
    "        self.log(\"train_loss\", loss,on_step = False ,on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        generated_images = self.get_output(batch)\n",
    "        loss = self.criterion(generated_images, batch['image'].to(self.models[\"accelerator\"].device))\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        generated_images = self.get_output(batch)\n",
    "        loss = self.criterion(generated_images, batch['image'].to(self.models[\"accelerator\"].device))\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        \n",
    "        for i in range(len(generated_images)):\n",
    "            x_sample = pil_to_tensor(generated_images[i])\n",
    "            torchvision.utils.save_image(x_sample,os.path.join(args[\"output_dir\"],batch['im_name'][i]))\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim =  torch.optim.Adam(lr = self.config['lr'], weight_decay = self.config['weight_decay'])   # https://pytorch.org/docs/stable/optim.html\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=3, factor=0.7, \n",
    "                                                                  threshold=0.005, cooldown =2,verbose=True)\n",
    "        # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optim,gamma = 0.995 ,last_epoch=-1,   verbose=True)\n",
    "\n",
    "        return [optim], [{'scheduler': lr_scheduler, 'interval': 'epoch', 'monitor': 'train_loss', 'name': 'lr_scheduler'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar, RichModelSummary\n",
    "from torchvision import transforms\n",
    "\n",
    "from  pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.00001,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "theme = RichProgressBarTheme(metrics='green', time='yellow', progress_bar_finished='#8c53e0' ,progress_bar='#c99e38')\n",
    "rich_progress_bar = RichProgressBar(theme=theme)\n",
    "\n",
    "# rich_model_summary = RichModelSummary(max_depth=5)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=3,\n",
    "    verbose=True,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import yaml \n",
    "\n",
    "with open('viton_train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "#___________________________________________________________________________________________________________________\n",
    "model = TrainEval()\n",
    "\n",
    "NAME = config['model_name']\n",
    "checkpoint_callback.dirpath = os.path.join(config['dir'], 'ckpts')\n",
    "checkpoint_callback.filename = NAME+'__' + config['ckpt_file_name']\n",
    "\n",
    "run_name = f\"lr_{config['lr']} *** bs{config['BATCH_SIZE']} *** decay_{config['weight_decay']}\"\n",
    "wandb_logger = WandbLogger(project= NAME, name = run_name)\n",
    "\n",
    "trainer = Trainer(callbacks=[early_stop_callback, checkpoint_callback, rich_progress_bar], \n",
    "                  accelerator = 'gpu' ,max_epochs=args.train_epochs, logger=[wandb_logger])  \n",
    " \n",
    "trainer.fit(model, train_dataloader, test_dataloader)\n",
    "trainer.test(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
